{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0d708b92321763157f37de519d81887a0eb2dbc14183f20cda4a0707ff0a2fa51",
   "display_name": "Python 3.9.4 64-bit ('my_nlp_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18을 위해 최대한 간단히 수정한 BasicBlock 클래스 정의\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # 3x3 필터를 사용 (너비와 높이를 줄일 때는 stride 값 조절)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes) # 배치 정규화(batch normalization)\n",
    "\n",
    "        # 3x3 필터를 사용 (패딩을 1만큼 주기 때문에 너비와 높이가 동일)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes) # 배치 정규화(batch normalization)\n",
    "\n",
    "        self.shortcut = nn.Sequential() # identity인 경우\n",
    "        if stride != 1: # stride가 1이 아니라면, Identity mapping이 아닌 경우\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x) # (핵심) skip connection\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 클래스 정의\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        # 64개의 3x3 필터(filter)를 사용\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes # 다음 레이어를 위해 채널 수 변경\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# ResNet18 함수 정의\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])        "
   ]
  },
  {
   "source": [
    "mnist 데이터셋(Dataset) 다운로드 및 불러오기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "9920512it [04:51, 37115.40it/s]                             Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[ADownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "\n",
      "  0%|          | 0/28881 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[AExtracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "\n",
      "\n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 16384/1648877 [00:00<00:24, 66804.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 24576/1648877 [00:00<00:34, 47626.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 32768/1648877 [00:01<00:38, 41514.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 40960/1648877 [00:01<00:41, 38838.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 49152/1648877 [00:01<00:42, 37672.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 57344/1648877 [00:01<00:43, 36237.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 65536/1648877 [00:02<00:44, 35694.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 73728/1648877 [00:02<00:44, 35123.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 81920/1648877 [00:02<00:44, 34839.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 90112/1648877 [00:02<00:45, 34636.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 98304/1648877 [00:03<00:45, 34418.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 106496/1648877 [00:03<00:44, 34312.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 114688/1648877 [00:03<00:44, 34250.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 122880/1648877 [00:03<00:44, 34225.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 131072/1648877 [00:04<00:44, 34176.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 139264/1648877 [00:04<00:44, 34142.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 147456/1648877 [00:04<00:44, 34013.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 155648/1648877 [00:04<00:43, 34043.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 163840/1648877 [00:05<00:43, 34076.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 172032/1648877 [00:05<00:43, 34147.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 180224/1648877 [00:05<00:43, 33939.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 188416/1648877 [00:05<00:42, 34150.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 196608/1648877 [00:05<00:43, 33769.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 204800/1648877 [00:06<00:42, 34222.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 212992/1648877 [00:06<00:42, 33924.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 221184/1648877 [00:06<00:41, 34134.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 229376/1648877 [00:06<00:41, 34092.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 237568/1648877 [00:07<00:42, 33483.04it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 245760/1648877 [00:07<00:42, 33050.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 253952/1648877 [00:07<00:42, 32637.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 262144/1648877 [00:07<00:42, 32319.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 270336/1648877 [00:08<00:42, 32241.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 278528/1648877 [00:08<00:42, 32237.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 286720/1648877 [00:08<00:41, 32728.88it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 294912/1648877 [00:08<00:40, 33121.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 303104/1648877 [00:09<00:40, 33456.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 311296/1648877 [00:09<00:39, 33607.48it/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 319488/1648877 [00:09<00:39, 33577.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 327680/1648877 [00:09<00:39, 33869.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 335872/1648877 [00:10<00:38, 33787.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 344064/1648877 [00:10<00:38, 34041.72it/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██▏       | 352256/1648877 [00:10<00:38, 33257.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 360448/1648877 [00:10<00:37, 34312.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 368640/1648877 [00:11<00:38, 33451.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 376832/1648877 [00:11<00:37, 34290.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 385024/1648877 [00:11<00:37, 33666.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 393216/1648877 [00:11<00:36, 34395.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 401408/1648877 [00:12<00:36, 34269.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 409600/1648877 [00:12<00:36, 34177.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 417792/1648877 [00:12<00:35, 34241.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 425984/1648877 [00:12<00:35, 34021.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 434176/1648877 [00:13<00:35, 34262.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 442368/1648877 [00:13<00:35, 33814.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 450560/1648877 [00:13<00:34, 34259.94it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 458752/1648877 [00:13<00:34, 34931.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 466944/1648877 [00:13<00:33, 35357.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 475136/1648877 [00:14<00:32, 35621.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 483328/1648877 [00:14<00:32, 35866.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 491520/1648877 [00:14<00:32, 36122.50it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 499712/1648877 [00:14<00:32, 35485.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 507904/1648877 [00:15<00:33, 34266.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 516096/1648877 [00:15<00:33, 33609.49it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 524288/1648877 [00:15<00:33, 33679.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 532480/1648877 [00:15<00:33, 33737.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 540672/1648877 [00:16<00:32, 33934.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 548864/1648877 [00:16<00:32, 33790.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 557056/1648877 [00:16<00:32, 34055.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 565248/1648877 [00:16<00:31, 34050.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 573440/1648877 [00:17<00:31, 33960.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 581632/1648877 [00:17<00:31, 33989.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "9920512it [05:10, 37115.40it/s]\n",
      "\n",
      " 36%|███▋      | 598016/1648877 [00:18<00:42, 24850.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 614400/1648877 [00:18<00:28, 36622.91it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 622592/1648877 [00:18<00:28, 35435.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 630784/1648877 [00:18<00:29, 34445.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 638976/1648877 [00:19<00:29, 34211.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 647168/1648877 [00:19<00:29, 34173.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 655360/1648877 [00:19<00:28, 34267.62it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 663552/1648877 [00:19<00:28, 34094.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 671744/1648877 [00:20<00:28, 34131.06it/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 679936/1648877 [00:20<00:28, 34154.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 688128/1648877 [00:20<00:28, 34145.66it/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 696320/1648877 [00:20<00:27, 34128.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 704512/1648877 [00:21<00:28, 33363.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 712704/1648877 [00:21<00:28, 32879.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▎     | 720896/1648877 [00:21<00:28, 32612.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 729088/1648877 [00:21<00:28, 32521.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 737280/1648877 [00:22<00:28, 32340.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 745472/1648877 [00:22<00:28, 32230.45it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 753664/1648877 [00:22<00:27, 32088.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 761856/1648877 [00:22<00:27, 32064.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 770048/1648877 [00:23<00:27, 32005.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 778240/1648877 [00:23<00:27, 31913.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 786432/1648877 [00:23<00:27, 31235.96it/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 794624/1648877 [00:23<00:26, 32169.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▊     | 802816/1648877 [00:24<00:26, 32084.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 811008/1648877 [00:24<00:26, 32100.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|████▉     | 819200/1648877 [00:24<00:25, 32041.24it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 827392/1648877 [00:24<00:25, 31995.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 835584/1648877 [00:25<00:25, 31908.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 843776/1648877 [00:25<00:25, 31725.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 851968/1648877 [00:25<00:24, 32106.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 860160/1648877 [00:25<00:24, 31879.75it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 868352/1648877 [00:26<00:24, 31846.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 876544/1648877 [00:26<00:24, 32046.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▎    | 884736/1648877 [00:26<00:23, 32061.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 892928/1648877 [00:26<00:23, 32018.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▍    | 901120/1648877 [00:27<00:23, 31849.90it/s]\u001b[A\u001b[A\n",
      "\n",
      " 55%|█████▌    | 909312/1648877 [00:27<00:23, 31933.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 917504/1648877 [00:27<00:22, 31885.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 925696/1648877 [00:27<00:22, 31973.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 933888/1648877 [00:28<00:22, 32069.18it/s]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 942080/1648877 [00:28<00:22, 31746.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 950272/1648877 [00:28<00:21, 32634.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 958464/1648877 [00:28<00:20, 32987.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▊    | 966656/1648877 [00:29<00:20, 33402.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 974848/1648877 [00:29<00:20, 33404.12it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|█████▉    | 983040/1648877 [00:29<00:19, 33699.98it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 991232/1648877 [00:29<00:19, 33915.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 999424/1648877 [00:30<00:19, 33667.40it/s]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 1007616/1648877 [00:30<00:18, 33971.46it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 1015808/1648877 [00:30<00:18, 34138.86it/s]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 1024000/1648877 [00:30<00:18, 34004.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 1032192/1648877 [00:31<00:18, 34147.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 1040384/1648877 [00:31<00:17, 34134.13it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▎   | 1048576/1648877 [00:31<00:17, 34131.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 1056768/1648877 [00:31<00:17, 34093.82it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 1064960/1648877 [00:32<00:17, 33964.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▌   | 1073152/1648877 [00:32<00:16, 34159.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 1081344/1648877 [00:32<00:16, 34113.63it/s]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 1089536/1648877 [00:32<00:16, 34050.70it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 1097728/1648877 [00:33<00:16, 34132.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 1105920/1648877 [00:33<00:15, 34080.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 1114112/1648877 [00:33<00:15, 33631.31it/s]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 1122304/1648877 [00:33<00:15, 34168.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▊   | 1130496/1648877 [00:33<00:15, 34075.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 1138688/1648877 [00:34<00:15, 33946.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|██████▉   | 1146880/1648877 [00:34<00:14, 34155.68it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 1155072/1648877 [00:34<00:14, 34128.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 1163264/1648877 [00:34<00:14, 34183.47it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 1171456/1648877 [00:35<00:14, 33429.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 1179648/1648877 [00:35<00:14, 32748.57it/s]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 1187840/1648877 [00:35<00:14, 32702.76it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 1196032/1648877 [00:35<00:13, 32520.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 1204224/1648877 [00:36<00:13, 32368.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▎  | 1212416/1648877 [00:36<00:13, 32251.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 1220608/1648877 [00:36<00:13, 32056.87it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▍  | 1228800/1648877 [00:36<00:13, 31829.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 75%|███████▌  | 1236992/1648877 [00:37<00:12, 32190.77it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 1245184/1648877 [00:37<00:12, 31941.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 1253376/1648877 [00:37<00:12, 31940.27it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 1261568/1648877 [00:38<00:12, 31951.73it/s]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 1269760/1648877 [00:38<00:11, 32077.30it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 1277952/1648877 [00:38<00:11, 32034.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 1286144/1648877 [00:38<00:11, 31969.38it/s]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 1294336/1648877 [00:39<00:11, 32004.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 1302528/1648877 [00:39<00:10, 31927.16it/s]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 1310720/1648877 [00:39<00:10, 32004.64it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|███████▉  | 1318912/1648877 [00:39<00:10, 31832.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 1327104/1648877 [00:40<00:10, 31925.80it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 1335296/1648877 [00:40<00:09, 31882.19it/s]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████▏ | 1343488/1648877 [00:40<00:09, 32054.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 1351680/1648877 [00:40<00:09, 31935.74it/s]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 1359872/1648877 [00:41<00:09, 31981.20it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 1368064/1648877 [00:41<00:08, 31945.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 1376256/1648877 [00:41<00:08, 31959.09it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 1384448/1648877 [00:41<00:08, 32024.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 1392640/1648877 [00:42<00:08, 31924.67it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▍ | 1400832/1648877 [00:42<00:07, 31689.25it/s]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 1409024/1648877 [00:42<00:07, 32422.84it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 1417216/1648877 [00:42<00:07, 32154.89it/s]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▋ | 1425408/1648877 [00:43<00:06, 33710.93it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 1433600/1648877 [00:43<00:06, 33869.65it/s]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 1441792/1648877 [00:43<00:06, 33480.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 1449984/1648877 [00:43<00:05, 33992.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 1458176/1648877 [00:44<00:05, 34183.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 1466368/1648877 [00:44<00:05, 34037.55it/s]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 1474560/1648877 [00:44<00:05, 34083.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|████████▉ | 1482752/1648877 [00:44<00:04, 34014.01it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 1490944/1648877 [00:45<00:04, 34137.00it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 1499136/1648877 [00:45<00:04, 34013.17it/s]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████▏| 1507328/1648877 [00:45<00:04, 34073.15it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 1515520/1648877 [00:45<00:03, 34107.28it/s]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 1523712/1648877 [00:46<00:03, 33197.26it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 1531904/1648877 [00:46<00:03, 34299.85it/s]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 1540096/1648877 [00:46<00:03, 33996.33it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 1548288/1648877 [00:46<00:02, 33860.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 1556480/1648877 [00:46<00:02, 34271.08it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▍| 1564672/1648877 [00:47<00:02, 34362.21it/s]\u001b[A\u001b[A\n",
      "\n",
      " 95%|█████████▌| 1572864/1648877 [00:47<00:02, 33998.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 1581056/1648877 [00:47<00:01, 34168.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▋| 1589248/1648877 [00:47<00:01, 33694.03it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 1597440/1648877 [00:48<00:01, 34376.05it/s]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 1605632/1648877 [00:48<00:01, 34204.11it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 1613824/1648877 [00:48<00:01, 34213.22it/s]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 1622016/1648877 [00:48<00:00, 33903.43it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 1630208/1648877 [00:49<00:00, 33501.02it/s]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 1638400/1648877 [00:49<00:00, 32998.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████▉| 1646592/1648877 [00:49<00:00, 32674.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "1654784it [00:49, 30241.91it/s]                             \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\u001b[AExtracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4542 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[AExtracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n",
      "C:\\Users\\PC\\anaconda3\\envs\\my_nlp_env\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ]
  },
  {
   "source": [
    "환경 설정 및 학습(Training) 함수 정의"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "net = torch.nn.DataParallel(net)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "learning_rate = 0.01\n",
    "file_name = 'resnet18_mnist.pt'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\n[ Train epoch: %d ]' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        benign_outputs = net(inputs)\n",
    "        loss = criterion(benign_outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = benign_outputs.max(1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\nCurrent batch:', str(batch_idx))\n",
    "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
    "            print('Current benign train loss:', loss.item())\n",
    "\n",
    "    print('\\nTotal benign train accuarcy:', 100. * correct / total)\n",
    "    print('Total benign train loss:', train_loss)\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    print('\\n[ Test epoch: %d ]' % epoch)\n",
    "    net.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        total += targets.size(0)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss += criterion(outputs, targets).item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print('\\nTest accuarcy:', 100. * correct / total)\n",
    "    print('Test average loss:', loss / total)\n",
    "\n",
    "    state = {\n",
    "        'net': net.state_dict()\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + file_name)\n",
    "    print('Model Saved!')\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 5:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "source": [
    "학습(Training) 진행\n",
    "\n",
    "MNIST 데이터셋에 대하여 전체 10 epoch로 99.5% test accuracy를 시연할 수 있습니다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[ Train epoch: 0 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.125\n",
      "Current benign train loss: 2.3776252269744873\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.027103595435619354\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.984375\n",
      "Current benign train loss: 0.04098495841026306\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.984375\n",
      "Current benign train loss: 0.04210031032562256\n",
      "\n",
      "Current batch: 400\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.01913333870470524\n",
      "\n",
      "Total benign train accuarcy: 96.13333333333334\n",
      "Total benign train loss: 57.923229936510324\n",
      "\n",
      "[ Test epoch: 0 ]\n",
      "\n",
      "Test accuarcy: 97.3\n",
      "Test average loss: 0.0008119056981639005\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 1 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.984375\n",
      "Current benign train loss: 0.038894835859537125\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0066114868968725204\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.984375\n",
      "Current benign train loss: 0.03037853352725506\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.9921875\n",
      "Current benign train loss: 0.024516068398952484\n",
      "\n",
      "Current batch: 400\n",
      "Current benign train accuracy: 0.984375\n",
      "Current benign train loss: 0.03825832158327103\n",
      "\n",
      "Total benign train accuarcy: 99.22666666666667\n",
      "Total benign train loss: 11.68937530007679\n",
      "\n",
      "[ Test epoch: 1 ]\n",
      "\n",
      "Test accuarcy: 98.8\n",
      "Test average loss: 0.0003738976272608852\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 2 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.984375\n",
      "Current benign train loss: 0.09372818470001221\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.006053942255675793\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0076242731884121895\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.003353606443852186\n",
      "\n",
      "Current batch: 400\n",
      "Current benign train accuracy: 0.9921875\n",
      "Current benign train loss: 0.01764816790819168\n",
      "\n",
      "Total benign train accuarcy: 99.56\n",
      "Total benign train loss: 6.756604913913179\n",
      "\n",
      "[ Test epoch: 2 ]\n",
      "\n",
      "Test accuarcy: 99.12\n",
      "Test average loss: 0.00026816487778633016\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 3 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.002433294663205743\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.9921875\n",
      "Current benign train loss: 0.012905885465443134\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.005226523149758577\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0068843611516058445\n",
      "\n",
      "Current batch: 400\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.002577038947492838\n",
      "\n",
      "Total benign train accuarcy: 99.74\n",
      "Total benign train loss: 4.00632842243067\n",
      "\n",
      "[ Test epoch: 3 ]\n",
      "\n",
      "Test accuarcy: 99.28\n",
      "Test average loss: 0.00024795306903379244\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 4 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.00047841446939855814\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0011117056710645556\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0053728679195046425\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.007067047990858555\n",
      "\n",
      "Current batch: 400\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0006794959190301597\n",
      "\n",
      "Total benign train accuarcy: 99.825\n",
      "Total benign train loss: 2.4601941647633794\n",
      "\n",
      "[ Test epoch: 4 ]\n",
      "\n",
      "Test accuarcy: 99.33\n",
      "Test average loss: 0.00020193267632857896\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 5 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0016161527018994093\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.003010765416547656\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.004871044773608446\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.00047438882756978273\n",
      "\n",
      "Current batch: 400\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0026004451792687178\n",
      "\n",
      "Total benign train accuarcy: 99.945\n",
      "Total benign train loss: 1.2888542421133025\n",
      "\n",
      "[ Test epoch: 5 ]\n",
      "\n",
      "Test accuarcy: 99.46\n",
      "Test average loss: 0.00015020833364724238\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 6 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0007510338327847421\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0006217967020347714\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.002606878988444805\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.004344558343291283\n",
      "\n",
      "Current batch: 400\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0002309806295670569\n",
      "\n",
      "Total benign train accuarcy: 99.98833333333333\n",
      "Total benign train loss: 0.6241778274270473\n",
      "\n",
      "[ Test epoch: 6 ]\n",
      "\n",
      "Test accuarcy: 99.55\n",
      "Test average loss: 0.0001448195164014578\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 7 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.000707810977473855\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0007994076004251838\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0006332190241664648\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.00032912532333284616\n",
      "\n",
      "Current batch: 400\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0006579491309821606\n",
      "\n",
      "Total benign train accuarcy: 99.99333333333334\n",
      "Total benign train loss: 0.4944056768945302\n",
      "\n",
      "[ Test epoch: 7 ]\n",
      "\n",
      "Test accuarcy: 99.54\n",
      "Test average loss: 0.00014458630459139384\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 8 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0007888100808486342\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0005318320472724736\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.00026647752383723855\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0034304908476769924\n",
      "\n",
      "Current batch: 400\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0011761568021029234\n",
      "\n",
      "Total benign train accuarcy: 99.99666666666667\n",
      "Total benign train loss: 0.40246944798127515\n",
      "\n",
      "[ Test epoch: 8 ]\n",
      "\n",
      "Test accuarcy: 99.54\n",
      "Test average loss: 0.0001444055697666954\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 9 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.00031312837381847203\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0013746882323175669\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.00018635968444868922\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0003051477833651006\n",
      "\n",
      "Current batch: 400\n",
      "Current benign train accuracy: 1.0\n",
      "Current benign train loss: 0.0018211582209914923\n",
      "\n",
      "Total benign train accuarcy: 99.995\n",
      "Total benign train loss: 0.37040075652475934\n",
      "\n",
      "[ Test epoch: 9 ]\n",
      "\n",
      "Test accuarcy: 99.52\n",
      "Test average loss: 0.00014444666412437074\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 10):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}